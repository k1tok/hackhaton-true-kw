{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXYDiIQYF0Gc",
        "outputId": "1b54b5c4-28a1-4954-c2ef-f5146355ca21"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "\n",
        "# === 1. Загрузка и подготовка данных ===\n",
        "with open('dataset_train.json', 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "data = [d for d in raw_data if 'consumption' in d and isinstance(d['consumption'], dict)]\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "months = [str(i) for i in range(1, 13)]\n",
        "for m in months:\n",
        "    df[f'consumption_{m}'] = df['consumption'].apply(lambda d: d.get(m, 0))\n",
        "\n",
        "df.drop(columns=['consumption', 'address'], inplace=True, errors='ignore')\n",
        "df['roomsCount'] = pd.to_numeric(df['roomsCount'], errors='coerce')\n",
        "df['residentsCount'] = pd.to_numeric(df['residentsCount'], errors='coerce')\n",
        "df['isCommercial'] = df['isCommercial'].astype(int)\n",
        "\n",
        "# === 2. Новые признаки ===\n",
        "cons_cols = [f'consumption_{m}' for m in months]\n",
        "df['total_consumption'] = df[cons_cols].sum(axis=1)\n",
        "df['mean_consumption'] = df[cons_cols].mean(axis=1)\n",
        "df['std_consumption'] = df[cons_cols].std(axis=1)\n",
        "df['max_consumption'] = df[cons_cols].max(axis=1)\n",
        "df['min_consumption'] = df[cons_cols].min(axis=1)\n",
        "df['monthly_delta'] = df[cons_cols].apply(lambda row: row.max() - row.min(), axis=1)\n",
        "df['cons_per_resident'] = df['total_consumption'] / (df['residentsCount'] + 1)\n",
        "df['cons_per_room'] = df['total_consumption'] / (df['roomsCount'] + 1)\n",
        "\n",
        "# === 3. Признаки и метки ===\n",
        "numeric_features = [\n",
        "    'roomsCount', 'residentsCount'\n",
        "] + cons_cols + [\n",
        "    'total_consumption', 'mean_consumption', 'std_consumption',\n",
        "    'max_consumption', 'min_consumption', 'monthly_delta',\n",
        "    'cons_per_resident', 'cons_per_room'\n",
        "]\n",
        "categorical_features = ['buildingType']\n",
        "\n",
        "X = df[numeric_features + categorical_features]\n",
        "y = df['isCommercial']\n",
        "\n",
        "# === 4. Тренировочный и валидационный набор ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# === 5. Препроцессинг ===\n",
        "numeric_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "categorical_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# === 6. Модель нейросети с улучшениями ===\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=5000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=lr_schedule),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n",
        ")\n",
        "\n",
        "# === 7. Обучение ===\n",
        "class_weights = {0: 1, 1: 2}\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_processed, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# === 8. Сохранение модели и препроцессора ===\n",
        "model.save('commercial_model.keras')\n",
        "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
        "\n",
        "print(\"\\n\\u2705 Обучение завершено. Модель и препроцессор сохранены.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXeAEs3_-1Vk"
      },
      "outputs": [],
      "source": [
        "def add_engineered_features(df):\n",
        "    # Переименуем consumption столбцы\n",
        "    for i in range(1, 13):\n",
        "        df.rename(columns={f\"month_{i}\": f\"consumption_{i}\"}, inplace=True)\n",
        "\n",
        "    month_cols = [f\"consumption_{i}\" for i in range(1, 13)]\n",
        "    consumption = df[month_cols]\n",
        "\n",
        "    df[\"total_consumption\"] = consumption.sum(axis=1)\n",
        "    df[\"mean_consumption\"] = consumption.mean(axis=1)\n",
        "    df[\"std_consumption\"] = consumption.std(axis=1)\n",
        "    df[\"min_consumption\"] = consumption.min(axis=1)\n",
        "    df[\"max_consumption\"] = consumption.max(axis=1)\n",
        "    df[\"monthly_delta\"] = df[\"max_consumption\"] - df[\"min_consumption\"]\n",
        "    df[\"cons_per_resident\"] = df[\"total_consumption\"] / df[\"residentsCount\"].replace(0, 1)\n",
        "    df[\"cons_per_room\"] = df[\"total_consumption\"] / df[\"roomsCount\"].replace(0, 1)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSLAqB57_eJ7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "def flatten_consumption(consumption_dict):\n",
        "    return [consumption_dict.get(str(month), 0) for month in range(1, 13)]\n",
        "\n",
        "def prepare_dataframe(data):\n",
        "    records = []\n",
        "    for entry in data:\n",
        "        flat = {\n",
        "            \"accountId\": entry[\"accountId\"],\n",
        "            \"roomsCount\": entry[\"roomsCount\"],\n",
        "            \"residentsCount\": entry[\"residentsCount\"],\n",
        "            \"buildingType\": entry[\"buildingType\"]\n",
        "        }\n",
        "        consumption_values = flatten_consumption(entry[\"consumption\"])\n",
        "        for i, val in enumerate(consumption_values, 1):\n",
        "            flat[f\"month_{i}\"] = val\n",
        "        records.append(flat)\n",
        "    df = pd.DataFrame(records)\n",
        "    df = add_engineered_features(df)  # Важно!\n",
        "    return df\n",
        "\n",
        "def predict_commercial(input_json_path, output_json_path):\n",
        "    model = load_model(\"commercial_model.keras\")\n",
        "    preprocessor = joblib.load(\"preprocessor.pkl\")\n",
        "\n",
        "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
        "        new_data = json.load(f)\n",
        "\n",
        "    df = prepare_dataframe(new_data)\n",
        "    account_ids = df[\"accountId\"].tolist()\n",
        "    X = df.drop(columns=[\"accountId\"])\n",
        "\n",
        "    X_processed = preprocessor.transform(X)\n",
        "    probabilities = model.predict(X_processed).flatten()\n",
        "    predictions = probabilities >= 0.5\n",
        "\n",
        "    results = [\n",
        "        {\n",
        "            \"accountId\": acc_id,\n",
        "            \"isCommercial\": bool(pred),\n",
        "            \"probability\": round(float(prob), 4)\n",
        "        }\n",
        "        for acc_id, pred, prob in sorted(zip(account_ids, predictions, probabilities), key=lambda x: x[2], reverse=True)\n",
        "    ]\n",
        "\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved predictions to {output_json_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l3jpgG_91YS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def flatten_consumption(consumption_dict):\n",
        "    return [consumption_dict.get(str(month), 0) for month in range(1, 13)]\n",
        "\n",
        "def prepare_dataframe(data):\n",
        "    records = []\n",
        "    for entry in data:\n",
        "        flat = {\n",
        "            \"accountId\": entry.get(\"accountId\"),\n",
        "            \"roomsCount\": entry.get(\"roomsCount\"),\n",
        "            \"residentsCount\": entry.get(\"residentsCount\", 0),\n",
        "            \"buildingType\": entry.get(\"buildingType\")\n",
        "        }\n",
        "        consumption_values = flatten_consumption(entry.get(\"consumption\", []))\n",
        "        # month → consumption\n",
        "        for i, val in enumerate(consumption_values, 1):\n",
        "            flat[f\"consumption_{i}\"] = val\n",
        "        records.append(flat)\n",
        "    df = pd.DataFrame(records)\n",
        "    # Добавляем  признаки\n",
        "    df = add_engineered_features(df)\n",
        "    return df\n",
        "\n",
        "def predict_commercial(input_json_path, output_json_path):\n",
        "    # Load model and preprocessor\n",
        "    model = load_model(\"commercial_model.keras\")\n",
        "    preprocessor = joblib.load(\"preprocessor.pkl\")\n",
        "\n",
        "    # Load new data\n",
        "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
        "        new_data = json.load(f)\n",
        "\n",
        "    # Prepare dataframe\n",
        "    df = prepare_dataframe(new_data)\n",
        "    account_ids = df[\"accountId\"].tolist()\n",
        "    X = df.drop(columns=[\"accountId\"])\n",
        "\n",
        "    print(\" Фактические признаки:\", X.columns.tolist())\n",
        "    print(\" Ожидаемые признаки:\", preprocessor.transformers_[0][2])\n",
        "\n",
        "\n",
        "    # Preprocess features\n",
        "    X_processed = preprocessor.transform(X)\n",
        "\n",
        "    # Predict probabilities\n",
        "    probabilities = model.predict(X_processed).flatten()\n",
        "    predictions = probabilities >= 0.5\n",
        "\n",
        "    # Prepare output\n",
        "    results = [\n",
        "        {\n",
        "            \"accountId\": acc_id,\n",
        "            \"isCommercial\": bool(pred),\n",
        "            \"probability\": round(float(prob), 4)\n",
        "        }\n",
        "        for acc_id, pred, prob in sorted(zip(account_ids, predictions, probabilities), key=lambda x: x[2], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved predictions to {output_json_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXUzMOe4Dpk8",
        "outputId": "4eae830e-8b0a-40f8-d3c7-f3304bf50184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Фактические признаки: ['roomsCount', 'residentsCount', 'buildingType', 'consumption_1', 'consumption_2', 'consumption_3', 'consumption_4', 'consumption_5', 'consumption_6', 'consumption_7', 'consumption_8', 'consumption_9', 'consumption_10', 'consumption_11', 'consumption_12', 'total_consumption', 'mean_consumption', 'std_consumption', 'min_consumption', 'max_consumption', 'monthly_delta', 'cons_per_resident', 'cons_per_room']\n",
            " Ожидаемые признаки: ['roomsCount', 'residentsCount', 'consumption_1', 'consumption_2', 'consumption_3', 'consumption_4', 'consumption_5', 'consumption_6', 'consumption_7', 'consumption_8', 'consumption_9', 'consumption_10', 'consumption_11', 'consumption_12', 'total_consumption', 'mean_consumption', 'std_consumption', 'max_consumption', 'min_consumption', 'monthly_delta', 'cons_per_resident', 'cons_per_room']\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "Saved predictions to prediction_results.json\n"
          ]
        }
      ],
      "source": [
        "predict_commercial('new_data.json', 'prediction_results.json')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
